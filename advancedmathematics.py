# -*- coding: utf-8 -*-
"""AdvancedMathematics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L6s8BI9gIwjEiM5f4_ZpHEDVaJKi4gP7
"""

import pandas as pd
import numpy as np

df = pd.read_csv("data.csv", encoding='latin1')

x = df['no2'].dropna().values

##(derived from previous assignment)
a_r = 1.0
b_r = 1.5


z = x + a_r * np.sin(b_r * x)

print("Sample z values:")
print(z[:10])

!pip install torch

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

z_tensor = torch.tensor(z, dtype=torch.float32).view(-1,1)

z_mean = z_tensor.mean()
z_std = z_tensor.std()

z_tensor = (z_tensor - z_mean) / z_std

class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(1,16),
            nn.ReLU(),
            nn.Linear(16,16),
            nn.ReLU(),
            nn.Linear(16,1)
        )

    def forward(self, x):
        return self.model(x)

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(1,16),
            nn.LeakyReLU(0.2),
            nn.Linear(16,16),
            nn.LeakyReLU(0.2),
            nn.Linear(16,1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

G = Generator()
D = Discriminator()

criterion = nn.BCELoss()

opt_G = optim.Adam(G.parameters(), lr=0.001)
opt_D = optim.Adam(D.parameters(), lr=0.001)

epochs = 2000
batch_size = 64

for epoch in range(epochs):

    idx = torch.randint(0, z_tensor.size(0), (batch_size,))
    real = z_tensor[idx]

    noise = torch.randn(batch_size,1)
    fake = G(noise).detach()

    real_labels = torch.ones(batch_size,1)
    fake_labels = torch.zeros(batch_size,1)

    loss_real = criterion(D(real), real_labels)
    loss_fake = criterion(D(fake), fake_labels)

    loss_D = loss_real + loss_fake

    opt_D.zero_grad()
    loss_D.backward()
    opt_D.step()

    noise = torch.randn(batch_size,1)
    fake = G(noise)

    loss_G = criterion(D(fake), real_labels)

    opt_G.zero_grad()
    loss_G.backward()
    opt_G.step()

    if epoch % 200 == 0:
        print(f"Epoch {epoch} | D Loss: {loss_D.item():.4f} | G Loss: {loss_G.item():.4f}")

G.eval()

num_samples = 5000
noise = torch.randn(num_samples, 1)

fake_samples = G(noise).detach().numpy()

fake_samples = fake_samples * z_std.numpy() + z_mean.numpy()

!pip install scikit-learn

from sklearn.neighbors import KernelDensity
import numpy as np
import matplotlib.pyplot as plt


kde = KernelDensity(kernel='gaussian', bandwidth=0.5)
kde.fit(fake_samples)

z_range = np.linspace(min(fake_samples), max(fake_samples), 500).reshape(-1,1)

log_density = kde.score_samples(z_range)
density = np.exp(log_density)


plt.figure(figsize=(8,5))
plt.plot(z_range, density, label="GAN Learned PDF")
plt.title("PDF Learned using GAN")
plt.xlabel("z")
plt.ylabel("Density")
plt.legend()
plt.show()